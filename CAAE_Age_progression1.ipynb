{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoDKxCCfTGYh",
        "outputId": "e9172e75-25f4-4287-e368-64161b46bf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face-aging-CAAE'...\n",
            "remote: Enumerating objects: 424, done.\u001b[K\n",
            "remote: Total 424 (delta 0), reused 0 (delta 0), pack-reused 424 (from 1)\u001b[K\n",
            "Receiving objects: 100% (424/424), 195.22 MiB | 23.14 MiB/s, done.\n",
            "Resolving deltas: 100% (229/229), done.\n",
            "Updating files: 100% (29/29), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ZhaoJ9014/face-aging-CAAE.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install 2to3 tool if not available\n",
        "!pip install -q 2to3\n",
        "# Convert the code from Python 2 to Python 3\n",
        "!2to3 -W -n -f all -o py3_converted face-aging-CAAE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYQg6KWGTG_W",
        "outputId": "cca37001-62e6-4478-a47b-0e474426812b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --write-unchanged-files/-W implies -w.\n",
            "lib2to3.main: Output in 'py3_converted' will mirror the input directory 'face-aging-CAAE' layout.\n",
            "RefactoringTool: Skipping optional fixer: buffer\n",
            "RefactoringTool: Skipping optional fixer: idioms\n",
            "RefactoringTool: Skipping optional fixer: set_literal\n",
            "RefactoringTool: Skipping optional fixer: ws_comma\n",
            "RefactoringTool: Refactored face-aging-CAAE/FaceAging.py\n",
            "--- face-aging-CAAE/FaceAging.py\t(original)\n",
            "+++ face-aging-CAAE/FaceAging.py\t(refactored)\n",
            "@@ -7,7 +7,7 @@\n",
            " # Please cite above paper if you use this code\n",
            " #\n",
            " \n",
            "-from __future__ import division\n",
            "+\n",
            " import os\n",
            " import time\n",
            " from glob import glob\n",
            "@@ -73,7 +73,7 @@\n",
            "             name='z_prior'\n",
            "         )\n",
            "         # ************************************* build the graph *******************************************************\n",
            "-        print '\\n\\tBuilding graph ...'\n",
            "+        print('\\n\\tBuilding graph ...')\n",
            " \n",
            "         # encoder: input image --> z\n",
            "         self.z = self.encoder(\n",
            "@@ -408,16 +408,16 @@\n",
            "                     }\n",
            "                 )\n",
            " \n",
            "-                print(\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "-                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV))\n",
            "-                print(\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err))\n",
            "-                print(\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err))\n",
            "+                print((\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "+                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV)))\n",
            "+                print((\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err)))\n",
            "+                print((\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err)))\n",
            " \n",
            "                 # estimate left run time\n",
            "                 elapse = time.time() - start_time\n",
            "                 time_left = ((num_epochs - epoch - 1) * num_batches + (num_batches - ind_batch - 1)) * elapse\n",
            "-                print(\"\\tTime left: %02d:%02d:%02d\" %\n",
            "-                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60))\n",
            "+                print((\"\\tTime left: %02d:%02d:%02d\" %\n",
            "+                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60)))\n",
            " \n",
            "                 # add to summary\n",
            "                 summary = self.summary.eval(\n",
            "@@ -708,7 +708,7 @@\n",
            "         num_samples = int(np.sqrt(self.size_batch))\n",
            "         file_names = glob(testing_samples_dir)\n",
            "         if len(file_names) < num_samples:\n",
            "-            print 'The number of testing images is must larger than %d' % num_samples\n",
            "+            print('The number of testing images is must larger than %d' % num_samples)\n",
            "             exit(0)\n",
            "         sample_files = file_names[0:num_samples]\n",
            "         sample = [load_image(\n",
            "@@ -736,6 +736,6 @@\n",
            "         self.test(images, gender_male, 'test_as_male.png')\n",
            "         self.test(images, gender_female, 'test_as_female.png')\n",
            " \n",
            "-        print '\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png')\n",
            "-\n",
            "-\n",
            "+        print('\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png'))\n",
            "+\n",
            "+\n",
            "RefactoringTool: Writing converted face-aging-CAAE/FaceAging.py to py3_converted/FaceAging.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/main.py\n",
            "--- face-aging-CAAE/main.py\t(original)\n",
            "+++ face-aging-CAAE/main.py\t(refactored)\n",
            "@@ -43,23 +43,23 @@\n",
            "             dataset_name=FLAGS.dataset  # name of the dataset in the folder ./data\n",
            "         )\n",
            "         if FLAGS.is_train:\n",
            "-            print '\\n\\tTraining Mode'\n",
            "+            print('\\n\\tTraining Mode')\n",
            "             if not FLAGS.use_trained_model:\n",
            "-                print '\\n\\tPre-train the network'\n",
            "+                print('\\n\\tPre-train the network')\n",
            "                 model.train(\n",
            "                     num_epochs=10,  # number of epochs\n",
            "                     use_trained_model=FLAGS.use_trained_model,\n",
            "                     use_init_model=FLAGS.use_init_model,\n",
            "                     weigts=(0, 0, 0)\n",
            "                 )\n",
            "-                print '\\n\\tPre-train is done! The training will start.'\n",
            "+                print('\\n\\tPre-train is done! The training will start.')\n",
            "             model.train(\n",
            "                 num_epochs=FLAGS.epoch,  # number of epochs\n",
            "                 use_trained_model=FLAGS.use_trained_model,\n",
            "                 use_init_model=FLAGS.use_init_model\n",
            "             )\n",
            "         else:\n",
            "-            print '\\n\\tTesting Mode'\n",
            "+            print('\\n\\tTesting Mode')\n",
            "             model.custom_test(\n",
            "                 testing_samples_dir=FLAGS.testdir + '/*jpg'\n",
            "             )\n",
            "RefactoringTool: Writing converted face-aging-CAAE/main.py to py3_converted/main.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/ops.py\n",
            "--- face-aging-CAAE/ops.py\t(original)\n",
            "+++ face-aging-CAAE/ops.py\t(refactored)\n",
            "@@ -1,4 +1,4 @@\n",
            "-from __future__ import division\n",
            "+\n",
            " import tensorflow as tf\n",
            " import numpy as np\n",
            " from scipy.misc import imread, imresize, imsave\n",
            "RefactoringTool: Writing converted face-aging-CAAE/ops.py to py3_converted/ops.py.\n",
            "RefactoringTool: No changes to face-aging-CAAE/init_model/__init__.py\n",
            "RefactoringTool: Writing converted face-aging-CAAE/init_model/__init__.py to py3_converted/init_model/__init__.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/init_model/zip_opt.py\n",
            "--- face-aging-CAAE/init_model/zip_opt.py\t(original)\n",
            "+++ face-aging-CAAE/init_model/zip_opt.py\t(refactored)\n",
            "@@ -53,6 +53,6 @@\n",
            "     try:\n",
            "         join('init_model/model_parts', 'init_model/model-init.data-00000-of-00001')\n",
            "     except:\n",
            "-        print 'Error joining files:'\n",
            "+        print('Error joining files:')\n",
            "         \n",
            "        \n",
            "RefactoringTool: Writing converted face-aging-CAAE/init_model/zip_opt.py to py3_converted/init_model/zip_opt.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/old_version/FaceAging.py\n",
            "--- face-aging-CAAE/old_version/FaceAging.py\t(original)\n",
            "+++ face-aging-CAAE/old_version/FaceAging.py\t(refactored)\n",
            "@@ -7,7 +7,7 @@\n",
            " # Please cite above paper if you use this code\n",
            " #\n",
            " \n",
            "-from __future__ import division\n",
            "+\n",
            " import os\n",
            " import time\n",
            " from glob import glob\n",
            "@@ -73,7 +73,7 @@\n",
            "             name='z_prior'\n",
            "         )\n",
            "         # ************************************* build the graph *******************************************************\n",
            "-        print '\\n\\tBuilding graph ...'\n",
            "+        print('\\n\\tBuilding graph ...')\n",
            " \n",
            "         # encoder: input image --> z\n",
            "         self.z = self.encoder(\n",
            "@@ -302,7 +302,7 @@\n",
            "             sample_label_gender[i, gender] = self.image_value_range[-1]\n",
            " \n",
            "         # ******************************************* training *******************************************************\n",
            "-        print '\\n\\tPreparing for training ...'\n",
            "+        print('\\n\\tPreparing for training ...')\n",
            " \n",
            "         # initialize the graph\n",
            "         tf.global_variables_initializer().run()\n",
            "@@ -397,16 +397,16 @@\n",
            "                     }\n",
            "                 )\n",
            " \n",
            "-                print(\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "-                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV))\n",
            "-                print(\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err))\n",
            "-                print(\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err))\n",
            "+                print((\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "+                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV)))\n",
            "+                print((\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err)))\n",
            "+                print((\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err)))\n",
            " \n",
            "                 # estimate left run time\n",
            "                 elapse = time.time() - start_time\n",
            "                 time_left = ((num_epochs - epoch - 1) * num_batches + (num_batches - ind_batch - 1)) * elapse\n",
            "-                print(\"\\tTime left: %02d:%02d:%02d\" %\n",
            "-                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60))\n",
            "+                print((\"\\tTime left: %02d:%02d:%02d\" %\n",
            "+                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60)))\n",
            " \n",
            "                 # add to summary\n",
            "                 summary = self.summary.eval(\n",
            "@@ -690,7 +690,7 @@\n",
            "         num_samples = int(np.sqrt(self.size_batch))\n",
            "         file_names = glob(testing_samples_dir)\n",
            "         if len(file_names) < num_samples:\n",
            "-            print 'The number of testing images is must larger than %d' % num_samples\n",
            "+            print('The number of testing images is must larger than %d' % num_samples)\n",
            "             exit(0)\n",
            "         sample_files = file_names[0:num_samples]\n",
            "         sample = [load_image(\n",
            "@@ -718,6 +718,6 @@\n",
            "         self.test(images, gender_male, 'test_as_male.png')\n",
            "         self.test(images, gender_female, 'test_as_female.png')\n",
            " \n",
            "-        print '\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png')\n",
            "-\n",
            "-\n",
            "+        print('\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png'))\n",
            "+\n",
            "+\n",
            "RefactoringTool: Writing converted face-aging-CAAE/old_version/FaceAging.py to py3_converted/old_version/FaceAging.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/old_version/main.py\n",
            "--- face-aging-CAAE/old_version/main.py\t(original)\n",
            "+++ face-aging-CAAE/old_version/main.py\t(refactored)\n",
            "@@ -27,12 +27,12 @@\n",
            "             dataset_name=FLAGS.dataset  # name of the dataset in the folder ./data\n",
            "         )\n",
            "         if FLAGS.is_train:\n",
            "-            print '\\n\\tTraining Mode'\n",
            "+            print('\\n\\tTraining Mode')\n",
            "             model.train(\n",
            "                 num_epochs=FLAGS.epoch,  # number of epochs\n",
            "             )\n",
            "         else:\n",
            "-            print '\\n\\tTesting Mode'\n",
            "+            print('\\n\\tTesting Mode')\n",
            "             model.custom_test(\n",
            "                 testing_samples_dir=FLAGS.testdir + '/*jpg'\n",
            "             )\n",
            "RefactoringTool: Writing converted face-aging-CAAE/old_version/main.py to py3_converted/old_version/main.py.\n",
            "RefactoringTool: Refactored face-aging-CAAE/old_version/ops.py\n",
            "--- face-aging-CAAE/old_version/ops.py\t(original)\n",
            "+++ face-aging-CAAE/old_version/ops.py\t(refactored)\n",
            "@@ -1,4 +1,4 @@\n",
            "-from __future__ import division\n",
            "+\n",
            " import tensorflow as tf\n",
            " import numpy as np\n",
            " from scipy.misc import imread, imresize, imsave\n",
            "RefactoringTool: Writing converted face-aging-CAAE/old_version/ops.py to py3_converted/old_version/ops.py.\n",
            "RefactoringTool: Files that were modified:\n",
            "RefactoringTool: face-aging-CAAE/FaceAging.py\n",
            "RefactoringTool: face-aging-CAAE/main.py\n",
            "RefactoringTool: face-aging-CAAE/ops.py\n",
            "RefactoringTool: face-aging-CAAE/init_model/__init__.py\n",
            "RefactoringTool: face-aging-CAAE/init_model/zip_opt.py\n",
            "RefactoringTool: face-aging-CAAE/old_version/FaceAging.py\n",
            "RefactoringTool: face-aging-CAAE/old_version/main.py\n",
            "RefactoringTool: face-aging-CAAE/old_version/ops.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy Pillow opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3QlK9luTLcl",
        "outputId": "d3b0cefd-53a0-4bcc-fdb2-6f7189f34472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd py3_converted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx3IjvTiTN2c",
        "outputId": "50e901d2-d5cf-4e8f-bc52-5caa1f5532a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/py3_converted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "NhJwH5gBTVFi",
        "outputId": "0e67d250-9d43-4c39-dade-e61b5158d746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84095c79-1c6f-4264-bac9-22fb0801528f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-84095c79-1c6f-4264-bac9-22fb0801528f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.jpg to test.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs(\"test_images\", exist_ok=True)\n",
        "for fname in uploaded.keys():\n",
        "    shutil.move(fname, os.path.join(\"test_images\", fname))\n"
      ],
      "metadata": {
        "id": "NHkbZkrNTXk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/py3_converted/ops.py\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    code = file.read()\n",
        "\n",
        "code = code.replace(\n",
        "    \"from scipy.misc import imread, imresize, imsave\",\n",
        "    \"from PIL import Image\\nimport numpy as np\"\n",
        ")\n",
        "\n",
        "# Replace imread, imresize, imsave usage\n",
        "code = code.replace(\"imread(\", \"np.array(Image.open(\")\n",
        "code = code.replace(\"imresize(\", \"np.array(Image.open(\")\n",
        "code = code.replace(\"imsave(\", \"Image.fromarray(\")\n",
        "\n",
        "# Patch any .astype('uint8') after resizing\n",
        "code = code.replace(\".astype('uint8')\", \")\")\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "print(\"✅ Patched deprecated scipy.misc calls in ops.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICpqvnkSTtxj",
        "outputId": "26715431-2bf0-41ab-8f0a-99585ef93343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched deprecated scipy.misc calls in ops.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 100 /content/py3_converted/ops.py | tail -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xp7cSnJT6xS",
        "outputId": "089636a2-239b-4114-b5eb-231bc27c38bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    elif len(x_shape) == 4:\n",
            "        label = tf.reshape(label, [x_shape[0], 1, 1, label_shape[-1]])\n",
            "        return tf.concat(axis=3, values=[x, label*tf.ones([x_shape[0], x_shape[1], x_shape[2], label_shape[-1]])])\n",
            "\n",
            "\n",
            "def load_image(\n",
            "        image_path,  # path of a image\n",
            "        image_size=64,  # expected size of the image\n",
            "        image_value_range=(-1, 1),  # expected pixel value range of the image\n",
            "        is_gray=False,  # gray scale or color image\n",
            "):\n",
            "    if is_gray:\n",
            "        image = np.array(Image.open(image_path, mode='L').astype(np.float32)\n",
            "    else:\n",
            "        image = np.array(Image.open(image_path, mode='RGB').astype(np.float32)\n",
            "    image = np.array(Image.open(image, [image_size, image_size])\n",
            "    image = image.astype(np.float32) * (image_value_range[-1] - image_value_range[0]) / 255.0 + image_value_range[0]\n",
            "    return image\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/py3_converted/ops.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Print around the problematic area\n",
        "for i, line in enumerate(lines[85:105], start=85):\n",
        "    print(f\"{i:>3}: {line.rstrip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Q1QCCWUBPT",
        "outputId": "6077ed3b-0f4f-4689-c65a-394e0f8e068d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 85: def load_image(\n",
            " 86:         image_path,  # path of a image\n",
            " 87:         image_size=64,  # expected size of the image\n",
            " 88:         image_value_range=(-1, 1),  # expected pixel value range of the image\n",
            " 89:         is_gray=False,  # gray scale or color image\n",
            " 90: ):\n",
            " 91:     if is_gray:\n",
            " 92:         image = np.array(Image.open(image_path, mode='L').astype(np.float32)\n",
            " 93:     else:\n",
            " 94:         image = np.array(Image.open(image_path, mode='RGB').astype(np.float32)\n",
            " 95:     image = np.array(Image.open(image, [image_size, image_size])\n",
            " 96:     image = image.astype(np.float32) * (image_value_range[-1] - image_value_range[0]) / 255.0 + image_value_range[0]\n",
            " 97:     return image\n",
            " 98: \n",
            " 99: \n",
            "100: def save_batch_images(\n",
            "101:         batch_images,   # a batch of images\n",
            "102:         save_path,  # path to save the images\n",
            "103:         image_value_range=(-1,1),   # value range of the input batch images\n",
            "104:         size_frame=None     # size of the image matrix, number of images in each row and column\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_code = \"\"\"\n",
        "def load_image(\n",
        "        image_path,  # path of a image\n",
        "        image_size=64,  # expected size of the image\n",
        "        image_value_range=(-1, 1),  # expected pixel value range of the image\n",
        "        is_gray=False,  # gray scale or color image\n",
        "):\n",
        "    if is_gray:\n",
        "        image = Image.open(image_path).convert('L')\n",
        "    else:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    image = image.resize((image_size, image_size))\n",
        "    image = np.array(image).astype(np.float32)\n",
        "    image = image * (image_value_range[-1] - image_value_range[0]) / 255.0 + image_value_range[0]\n",
        "    return image\n",
        "\"\"\"\n",
        "\n",
        "file_path = \"/content/py3_converted/ops.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Replace old load_image block\n",
        "start = 85\n",
        "end = 98\n",
        "lines[start:end+1] = correct_code.strip().split('\\n')\n",
        "\n",
        "# Save updated file\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write('\\n'.join(lines))\n",
        "\n",
        "print(\"✅ Fixed `load_image()` function in ops.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUrJfqheUSDD",
        "outputId": "f541468e-7ac5-463f-b579-ef03304a84af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed `load_image()` function in ops.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "V1Qry3a6VPzC",
        "outputId": "7fabcfbf-77d8-4496-d2ed-6c7a8e649cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Tensorflow 1 is unsupported in Colab.\n\nYour notebook should be updated to use Tensorflow 2.\nSee the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8d2919c1d33c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1.x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_tensorflow_magics.py\u001b[0m in \u001b[0;36m_tensorflow_version\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# pylint: disable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         textwrap.dedent(\n",
            "\u001b[0;31mValueError\u001b[0m: Tensorflow 1 is unsupported in Colab.\n\nYour notebook should be updated to use Tensorflow 2.\nSee the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -rn 'restore' /content/py3_converted/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxdFrsY2gXGV",
        "outputId": "be751d0c-ff6e-412b-f3c0-a63ee6c7c1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grep: /content/py3_converted/__pycache__/FaceAging.cpython-311.pyc: binary file matches\n",
            "/content/py3_converted/old_version/FaceAging.py:620:            self.saver.restore(self.session, os.path.join(checkpoint_dir, checkpoints_name))\r\n",
            "/content/py3_converted/FaceAging.py:647:                self.saver.restore(self.session, os.path.join(checkpoint_dir, checkpoints_name))\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"*.ckpt*\"\n"
      ],
      "metadata": {
        "id": "5Pqkm2A3hiIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/face-aging-CAAE -name \"main.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRHaHzz8kW9f",
        "outputId": "10261ca4-29b5-4a1c-8e3a-a5beef16842d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/face-aging-CAAE/main.py\n",
            "/content/face-aging-CAAE/old_version/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!2to3 -W -n -f all /content/face-aging-CAAE -o /content/face-aging-CAAE_py3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2PGMFJfks0K",
        "outputId": "f1ffe1d5-28a4-40f4-bfb7-906de93c7798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --write-unchanged-files/-W implies -w.\n",
            "lib2to3.main: Output in '/content/face-aging-CAAE_py3' will mirror the input directory '/content/face-aging-CAAE' layout.\n",
            "RefactoringTool: Skipping optional fixer: buffer\n",
            "RefactoringTool: Skipping optional fixer: idioms\n",
            "RefactoringTool: Skipping optional fixer: set_literal\n",
            "RefactoringTool: Skipping optional fixer: ws_comma\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/FaceAging.py\n",
            "--- /content/face-aging-CAAE/FaceAging.py\t(original)\n",
            "+++ /content/face-aging-CAAE/FaceAging.py\t(refactored)\n",
            "@@ -7,7 +7,7 @@\n",
            " # Please cite above paper if you use this code\n",
            " #\n",
            " \n",
            "-from __future__ import division\n",
            "+\n",
            " import os\n",
            " import time\n",
            " from glob import glob\n",
            "@@ -73,7 +73,7 @@\n",
            "             name='z_prior'\n",
            "         )\n",
            "         # ************************************* build the graph *******************************************************\n",
            "-        print '\\n\\tBuilding graph ...'\n",
            "+        print('\\n\\tBuilding graph ...')\n",
            " \n",
            "         # encoder: input image --> z\n",
            "         self.z = self.encoder(\n",
            "@@ -408,16 +408,16 @@\n",
            "                     }\n",
            "                 )\n",
            " \n",
            "-                print(\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "-                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV))\n",
            "-                print(\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err))\n",
            "-                print(\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err))\n",
            "+                print((\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "+                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV)))\n",
            "+                print((\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err)))\n",
            "+                print((\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err)))\n",
            " \n",
            "                 # estimate left run time\n",
            "                 elapse = time.time() - start_time\n",
            "                 time_left = ((num_epochs - epoch - 1) * num_batches + (num_batches - ind_batch - 1)) * elapse\n",
            "-                print(\"\\tTime left: %02d:%02d:%02d\" %\n",
            "-                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60))\n",
            "+                print((\"\\tTime left: %02d:%02d:%02d\" %\n",
            "+                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60)))\n",
            " \n",
            "                 # add to summary\n",
            "                 summary = self.summary.eval(\n",
            "@@ -708,7 +708,7 @@\n",
            "         num_samples = int(np.sqrt(self.size_batch))\n",
            "         file_names = glob(testing_samples_dir)\n",
            "         if len(file_names) < num_samples:\n",
            "-            print 'The number of testing images is must larger than %d' % num_samples\n",
            "+            print('The number of testing images is must larger than %d' % num_samples)\n",
            "             exit(0)\n",
            "         sample_files = file_names[0:num_samples]\n",
            "         sample = [load_image(\n",
            "@@ -736,6 +736,6 @@\n",
            "         self.test(images, gender_male, 'test_as_male.png')\n",
            "         self.test(images, gender_female, 'test_as_female.png')\n",
            " \n",
            "-        print '\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png')\n",
            "-\n",
            "-\n",
            "+        print('\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png'))\n",
            "+\n",
            "+\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/FaceAging.py to /content/face-aging-CAAE_py3/FaceAging.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/main.py\n",
            "--- /content/face-aging-CAAE/main.py\t(original)\n",
            "+++ /content/face-aging-CAAE/main.py\t(refactored)\n",
            "@@ -43,23 +43,23 @@\n",
            "             dataset_name=FLAGS.dataset  # name of the dataset in the folder ./data\n",
            "         )\n",
            "         if FLAGS.is_train:\n",
            "-            print '\\n\\tTraining Mode'\n",
            "+            print('\\n\\tTraining Mode')\n",
            "             if not FLAGS.use_trained_model:\n",
            "-                print '\\n\\tPre-train the network'\n",
            "+                print('\\n\\tPre-train the network')\n",
            "                 model.train(\n",
            "                     num_epochs=10,  # number of epochs\n",
            "                     use_trained_model=FLAGS.use_trained_model,\n",
            "                     use_init_model=FLAGS.use_init_model,\n",
            "                     weigts=(0, 0, 0)\n",
            "                 )\n",
            "-                print '\\n\\tPre-train is done! The training will start.'\n",
            "+                print('\\n\\tPre-train is done! The training will start.')\n",
            "             model.train(\n",
            "                 num_epochs=FLAGS.epoch,  # number of epochs\n",
            "                 use_trained_model=FLAGS.use_trained_model,\n",
            "                 use_init_model=FLAGS.use_init_model\n",
            "             )\n",
            "         else:\n",
            "-            print '\\n\\tTesting Mode'\n",
            "+            print('\\n\\tTesting Mode')\n",
            "             model.custom_test(\n",
            "                 testing_samples_dir=FLAGS.testdir + '/*jpg'\n",
            "             )\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/main.py to /content/face-aging-CAAE_py3/main.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/ops.py\n",
            "--- /content/face-aging-CAAE/ops.py\t(original)\n",
            "+++ /content/face-aging-CAAE/ops.py\t(refactored)\n",
            "@@ -1,4 +1,4 @@\n",
            "-from __future__ import division\n",
            "+\n",
            " import tensorflow as tf\n",
            " import numpy as np\n",
            " from scipy.misc import imread, imresize, imsave\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/ops.py to /content/face-aging-CAAE_py3/ops.py.\n",
            "RefactoringTool: No changes to /content/face-aging-CAAE/init_model/__init__.py\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/init_model/__init__.py to /content/face-aging-CAAE_py3/init_model/__init__.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/init_model/zip_opt.py\n",
            "--- /content/face-aging-CAAE/init_model/zip_opt.py\t(original)\n",
            "+++ /content/face-aging-CAAE/init_model/zip_opt.py\t(refactored)\n",
            "@@ -53,6 +53,6 @@\n",
            "     try:\n",
            "         join('init_model/model_parts', 'init_model/model-init.data-00000-of-00001')\n",
            "     except:\n",
            "-        print 'Error joining files:'\n",
            "+        print('Error joining files:')\n",
            "         \n",
            "        \n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/init_model/zip_opt.py to /content/face-aging-CAAE_py3/init_model/zip_opt.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/old_version/FaceAging.py\n",
            "--- /content/face-aging-CAAE/old_version/FaceAging.py\t(original)\n",
            "+++ /content/face-aging-CAAE/old_version/FaceAging.py\t(refactored)\n",
            "@@ -7,7 +7,7 @@\n",
            " # Please cite above paper if you use this code\n",
            " #\n",
            " \n",
            "-from __future__ import division\n",
            "+\n",
            " import os\n",
            " import time\n",
            " from glob import glob\n",
            "@@ -73,7 +73,7 @@\n",
            "             name='z_prior'\n",
            "         )\n",
            "         # ************************************* build the graph *******************************************************\n",
            "-        print '\\n\\tBuilding graph ...'\n",
            "+        print('\\n\\tBuilding graph ...')\n",
            " \n",
            "         # encoder: input image --> z\n",
            "         self.z = self.encoder(\n",
            "@@ -302,7 +302,7 @@\n",
            "             sample_label_gender[i, gender] = self.image_value_range[-1]\n",
            " \n",
            "         # ******************************************* training *******************************************************\n",
            "-        print '\\n\\tPreparing for training ...'\n",
            "+        print('\\n\\tPreparing for training ...')\n",
            " \n",
            "         # initialize the graph\n",
            "         tf.global_variables_initializer().run()\n",
            "@@ -397,16 +397,16 @@\n",
            "                     }\n",
            "                 )\n",
            " \n",
            "-                print(\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "-                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV))\n",
            "-                print(\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err))\n",
            "-                print(\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err))\n",
            "+                print((\"\\nEpoch: [%3d/%3d] Batch: [%3d/%3d]\\n\\tEG_err=%.4f\\tTV=%.4f\" %\n",
            "+                    (epoch+1, num_epochs, ind_batch+1, num_batches, EG_err, TV)))\n",
            "+                print((\"\\tEz=%.4f\\tDz=%.4f\\tDzp=%.4f\" % (Ez_err, Dz_err, Dzp_err)))\n",
            "+                print((\"\\tGi=%.4f\\tDi=%.4f\\tDiG=%.4f\" % (Gi_err, Di_err, DiG_err)))\n",
            " \n",
            "                 # estimate left run time\n",
            "                 elapse = time.time() - start_time\n",
            "                 time_left = ((num_epochs - epoch - 1) * num_batches + (num_batches - ind_batch - 1)) * elapse\n",
            "-                print(\"\\tTime left: %02d:%02d:%02d\" %\n",
            "-                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60))\n",
            "+                print((\"\\tTime left: %02d:%02d:%02d\" %\n",
            "+                      (int(time_left / 3600), int(time_left % 3600 / 60), time_left % 60)))\n",
            " \n",
            "                 # add to summary\n",
            "                 summary = self.summary.eval(\n",
            "@@ -690,7 +690,7 @@\n",
            "         num_samples = int(np.sqrt(self.size_batch))\n",
            "         file_names = glob(testing_samples_dir)\n",
            "         if len(file_names) < num_samples:\n",
            "-            print 'The number of testing images is must larger than %d' % num_samples\n",
            "+            print('The number of testing images is must larger than %d' % num_samples)\n",
            "             exit(0)\n",
            "         sample_files = file_names[0:num_samples]\n",
            "         sample = [load_image(\n",
            "@@ -718,6 +718,6 @@\n",
            "         self.test(images, gender_male, 'test_as_male.png')\n",
            "         self.test(images, gender_female, 'test_as_female.png')\n",
            " \n",
            "-        print '\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png')\n",
            "-\n",
            "-\n",
            "+        print('\\n\\tDone! Results are saved as %s\\n' % os.path.join(self.save_dir, 'test', 'test_as_xxx.png'))\n",
            "+\n",
            "+\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/old_version/FaceAging.py to /content/face-aging-CAAE_py3/old_version/FaceAging.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/old_version/main.py\n",
            "--- /content/face-aging-CAAE/old_version/main.py\t(original)\n",
            "+++ /content/face-aging-CAAE/old_version/main.py\t(refactored)\n",
            "@@ -27,12 +27,12 @@\n",
            "             dataset_name=FLAGS.dataset  # name of the dataset in the folder ./data\n",
            "         )\n",
            "         if FLAGS.is_train:\n",
            "-            print '\\n\\tTraining Mode'\n",
            "+            print('\\n\\tTraining Mode')\n",
            "             model.train(\n",
            "                 num_epochs=FLAGS.epoch,  # number of epochs\n",
            "             )\n",
            "         else:\n",
            "-            print '\\n\\tTesting Mode'\n",
            "+            print('\\n\\tTesting Mode')\n",
            "             model.custom_test(\n",
            "                 testing_samples_dir=FLAGS.testdir + '/*jpg'\n",
            "             )\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/old_version/main.py to /content/face-aging-CAAE_py3/old_version/main.py.\n",
            "RefactoringTool: Refactored /content/face-aging-CAAE/old_version/ops.py\n",
            "--- /content/face-aging-CAAE/old_version/ops.py\t(original)\n",
            "+++ /content/face-aging-CAAE/old_version/ops.py\t(refactored)\n",
            "@@ -1,4 +1,4 @@\n",
            "-from __future__ import division\n",
            "+\n",
            " import tensorflow as tf\n",
            " import numpy as np\n",
            " from scipy.misc import imread, imresize, imsave\n",
            "RefactoringTool: Writing converted /content/face-aging-CAAE/old_version/ops.py to /content/face-aging-CAAE_py3/old_version/ops.py.\n",
            "RefactoringTool: Files that were modified:\n",
            "RefactoringTool: /content/face-aging-CAAE/FaceAging.py\n",
            "RefactoringTool: /content/face-aging-CAAE/main.py\n",
            "RefactoringTool: /content/face-aging-CAAE/ops.py\n",
            "RefactoringTool: /content/face-aging-CAAE/init_model/__init__.py\n",
            "RefactoringTool: /content/face-aging-CAAE/init_model/zip_opt.py\n",
            "RefactoringTool: /content/face-aging-CAAE/old_version/FaceAging.py\n",
            "RefactoringTool: /content/face-aging-CAAE/old_version/main.py\n",
            "RefactoringTool: /content/face-aging-CAAE/old_version/ops.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/face-aging-CAAE_py3/main.py --is_train False --testdir /content/face-aging-CAAE_py3/test_images --savedir /content/face-aging-CAAE_py3/results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swwv5ZDkTdBt",
        "outputId": "5202d001-dc68-4b10-b2d2-79448794960c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 07:48:01.987597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744271282.025512   25599 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744271282.036928   25599 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2025-04-10 07:48:07.045853: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Namespace(is_train=True, epoch=50, dataset='UTKFace', savedir='save', testdir='None', use_trained_model=True, use_init_model=True)\n",
            "\n",
            "\tBuilding graph ...\n",
            "\n",
            "\tTraining Mode\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1744271290.133686   25599 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
            "\n",
            "\tLoading pre-trained model ...\n",
            "\tFAILED >_<!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 325, in train\n",
            "    join('init_model/model_parts', 'init_model/model-init.data-00000-of-00001')\n",
            "  File \"/content/face-aging-CAAE_py3/init_model/zip_opt.py\", line 31, in join\n",
            "    output = open(tofile, 'wb')\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'init_model/model-init.data-00000-of-00001'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 74, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 61, in main\n",
            "    model.train(\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 327, in train\n",
            "    raise Exception('Error joining files')\n",
            "Exception: Error joining files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/face-aging-CAAE_py3/main.py --is_train=False --testdir=/content/face-aging-CAAE_py3/test_images --savedir=/content/face-aging-CAAE_py3/results --use_trained_model=True --use_init_model=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oUFq5o40Uib",
        "outputId": "194f7ab1-28bd-4a6e-80b6-76477cef46fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 08:29:16.364827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744273756.392585   35487 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744273756.400872   35487 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2025-04-10 08:29:23.265675: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Namespace(is_train=True, epoch=50, dataset='UTKFace', savedir='save', testdir='None', use_trained_model=True, use_init_model=True)\n",
            "\n",
            "\tBuilding graph ...\n",
            "\n",
            "\tTraining Mode\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1744273767.502260   35487 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
            "\n",
            "\tLoading pre-trained model ...\n",
            "\tFAILED >_<!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 325, in train\n",
            "    join('init_model/model_parts', 'init_model/model-init.data-00000-of-00001')\n",
            "  File \"/content/face-aging-CAAE_py3/init_model/zip_opt.py\", line 31, in join\n",
            "    output = open(tofile, 'wb')\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'init_model/model-init.data-00000-of-00001'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 74, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 61, in main\n",
            "    model.train(\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 327, in train\n",
            "    raise Exception('Error joining files')\n",
            "Exception: Error joining files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/face-aging-CAAE/init_model/model_parts/* > /content/face-aging-CAAE/init_model/model-init.data-00000-of-00001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnDM8tJC0471",
        "outputId": "85c40003-37ea-430f-d1c3-637687b80b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/face-aging-CAAE/init_model/model_parts/save: Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/face-aging-CAAE_py3/init_model/model_parts\n"
      ],
      "metadata": {
        "id": "n1I2qrZF1O29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/face-aging-CAAE/init_model/model_parts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOvpcPps1pJb",
        "outputId": "54ebebf0-12ab-4bdd-c404-ae1901dfddfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/face-aging-CAAE/init_model/model_parts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/face-aging-CAAE/init_model/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k9Pe3IF1y5i",
        "outputId": "ecd83fd0-78f5-4bdd-fb4f-7ea1ba85cc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 180M\n",
            "-rw-r--r-- 1 root root   36 Apr 10 06:04 checkpoint\n",
            "-rw-r--r-- 1 root root    2 Apr 10 06:04 __init__.py\n",
            "-rw-r--r-- 1 root root 179M Apr 10 07:16 model-index.ckpt\n",
            "-rw-r--r-- 1 root root    0 Apr 10 08:35 model-init.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root 2.3K Apr 10 06:04 model-init.index\n",
            "-rw-r--r-- 1 root root 657K Apr 10 06:04 model-init.meta\n",
            "drwxr-xr-x 3 root root 4.0K Apr 10 08:35 model_parts\n",
            "-rw-r--r-- 1 root root 2.0K Apr 10 06:04 zip_opt.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/face-aging-CAAE/init_model/model_parts/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mjNTeJN2Fvg",
        "outputId": "3a623c2b-237d-4a88-97e3-68d070bd34ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 179M\n",
            "-rw-r--r-- 1 root root    0 Apr 10 08:36 model-init.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0001\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0002\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0003\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0004\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0005\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0006\n",
            "-rw-r--r-- 1 root root  24M Apr 10 06:04 part0007\n",
            "-rw-r--r-- 1 root root  15M Apr 10 06:04 part0008\n",
            "drwxr-xr-x 3 root root 4.0K Apr 10 07:48 save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/face-aging-CAAE_py3/main.py --is_train=False --testdir=/content/face-aging-CAAE_py3/test_images --savedir=/content/face-aging-CAAE_py3/results --use_trained_model=True --use_init_model=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mdh_jhFmqci",
        "outputId": "de189676-be45-4347-ace1-8dbb904cbf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 08:33:23.911674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744274003.935851   36505 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744274003.942829   36505 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2025-04-10 08:33:29.471947: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Namespace(is_train=True, epoch=50, dataset='UTKFace', savedir='save', testdir='None', use_trained_model=True, use_init_model=True)\n",
            "\n",
            "\tBuilding graph ...\n",
            "\n",
            "\tTraining Mode\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1744274012.648217   36505 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
            "\n",
            "\tLoading pre-trained model ...\n",
            "\tFAILED >_<!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 325, in train\n",
            "    join('init_model/model_parts', 'init_model/model-init.data-00000-of-00001')\n",
            "  File \"/content/face-aging-CAAE_py3/init_model/zip_opt.py\", line 31, in join\n",
            "    output = open(tofile, 'wb')\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'init_model/model-init.data-00000-of-00001'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 74, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/face-aging-CAAE_py3/main.py\", line 61, in main\n",
            "    model.train(\n",
            "  File \"/content/face-aging-CAAE_py3/FaceAging.py\", line 327, in train\n",
            "    raise Exception('Error joining files')\n",
            "Exception: Error joining files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/face-aging-CAAE/init_model/model_parts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CJ4fNhQTigD",
        "outputId": "c2c9b0ef-69bb-4876-baa2-e7c90e8da2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/face-aging-CAAE/init_model/model_parts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file part0001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22PPoMigieF5",
        "outputId": "dc114c6b-6ccf-4b56-bb6f-d002972f2a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part0001: data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/face-aging-CAAE/init_model/model_parts/part000* > /content/face-aging-CAAE/init_model/model_combined\n"
      ],
      "metadata": {
        "id": "SYxm9FvpjXhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try unzip\n",
        "!unzip /content/face-aging-CAAE/init_model/model_combined -d /content/face-aging-CAAE/init_model/unpacked_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9voUn8njk81",
        "outputId": "3f6a5f87-144e-4b7b-ae66-35c66e1467eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/face-aging-CAAE/init_model/model_combined\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/face-aging-CAAE/init_model/model_combined or\n",
            "        /content/face-aging-CAAE/init_model/model_combined.zip, and cannot find /content/face-aging-CAAE/init_model/model_combined.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try tar\n",
        "!tar -xvf /content/face-aging-CAAE/init_model/model_combined -C /content/face-aging-CAAE/init_model/unpacked_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYTq73SujycQ",
        "outputId": "701ce335-0491-4bf3-eb48-8237b187f325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: This does not look like a tar archive\n",
            "tar: Skipping to next header\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/face-aging-CAAE/init_model/model_combined /content/face-aging-CAAE/init_model/model-index.ckpt\n"
      ],
      "metadata": {
        "id": "sw3Q_it9j2E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHXn8tzbj6FU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}